１. 線形回帰モデル
・要点：
① 線形回帰モデル＝回帰問題を解くための機械学習モデル/教師あり学習
② モデル式　目的変数＝切片＋回帰係数x説明変数（次元数分）+誤差
③ パラメータ＝最小二乗法(学習データの平均二乗誤差を最小化）
④ 汎化性能測定＝データを学習データと検証データに分割し、学習データを使ってモデル作成後、検証データで性能評価
・実装演習結果：
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn import linear_model
import pandas as pd
import numpy as np
boston = load_boston()
y = boston.target
df = pd.DataFrame(boston.data, columns=boston.feature_names)
df['PRICE'] = y
crime_rm = df.loc[:, ['CRIM', 'RM']]
x = crime_rm
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)
model = linear_model.LinearRegression()
model.fit(x_train, y_train)
print(model.predict(np.array([[0.3, 4]])))

結果　[3.53965207]
・ステージテスト誤答考察：
Q線形回帰について誤っている説明は？
自分の回答　説明変数間に相関があると良い、推定ができない
実際の答え　訓練データが少ないほど過学習が起きやすい

間違えた理由　過学習と学習不足を同一のものとして捉えていた
過学習・・・訓練データが多く、訓練データへの適合率が高くなりすぎた状態
学習不足・・・訓練データが少なく、正しく分類できない状態

2. 非線形回帰モデル
・要点：
① 非線形回帰モデル(NonLinear Regression)＝線形で捉えることができない問題の解決方法
② 学習方法＝基底展開法（基底関数（多項式関数、ガウス型基底関数、スプライン関数など）＋パラメータベクトルの線形結合）
③ パラメータ＝最小二乗法/最尤法
④ 注意点＝未学習/過学習 ⇒　正則化によって改善
⑤ 正則化＝L2ノルム、L1ノルム
⑥ ホールドアウト法＝１回に学習するデータを絞り、学習回数を増やす
⑦ 交差検証＝データを学習用と評価用にn分割して、分割したモデルを評価して、精度の平均を取り最も良いモデルを採用

3.　ロジスティック回帰モデル
・要点：
① ロジスティック回帰モデル＝教師あり学習のクラス分類モデル
② シグモイド関数＝求めたい事象が起こる確率を出力
③ 数式　求めたい事象の確率＝シグモイド関数（切片＋回帰係数x説明変数）
④ 利用する確率分布＝ベルヌーイ分布（事象Ａの確率がpの場合、事象Ｂの確率は1-p)
⑤ 最尤推定＝尤度関数を最大化するようなパラメータを選ぶ推定方法
⑥ パラメータ更新方法＝勾配降下法、確率勾配降下法
⑦ 性能分析＝正解率、再現率、適合率、F値
・実装演習結果：
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np
df = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')
df = df.fillna(df.mean())
df_sex = pd.get_dummies(df['Sex'])
df_sex_age = df_sex.drop('female', axis = 1)
df_sex_age['Age'] = df['Age']
df_sex_age['Survived'] = df['Survived'] 
x = df_sex_age[['Age', 'male']]
y = df_sex_age['Survived']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)
lr = LogisticRegression()
lr.fit(x_train, y_train)
def survived(age, sex):
    if sex == 'male':
        sex = 0
    elif sex == 'female':
        sex = 1
    else:
        sex = nan
    if lr.predict(np.array([[age, sex]])) == 1:
        print('Alive')
    else:
        print('Dead')
survived(30, 'male')

結果　Alive
・ステージテスト誤答考察：
Q以下のデータをロジスティック回帰で分類した時の正しい説明とは？
データ　[x1, x2, y] = [0, 0, 1],[0, 1, 0],[1, 0, 0],[1, 1, 0]
答え　いずれも完全に分類できない
理由　選択肢はすべて線形の境界式だが、与えられたデータは非線形な境界でしか分類できないため


4. 主成分分析
・要点：
① 主成分分析＝多変量データの持つ構造を圧縮（データをベクトルに変形）
② 数式　線形変換後のベクトル＝データ行列x係数ベクトル（係数ベクトルを変えることで線形変換後のベクトルに変化をつけることができる）
③ 係数ベクトルの求め方＝ラグランジュ関数を最大化
④ ラグランジュ関数＝目的変数-ラグランジュ乗数x制約条件
⑤ 寄与率＝全情報量の内、該当する主成分が占める情報量の割合
・実装演習結果：
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
df = pd.read_csv('data.csv')
df.drop('Unnamed: 32', axis=1, inplace=True)
y = df.diagnosis.apply(lambda d: 1 if d == 'M' else 0)
x = df.loc[:, 'radius_mean':]
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)
logistic = LogisticRegression()
logistic.fit(x_train_scaled, y_train)
print(logistic.score(x_train_scaled, y_train))
print(logistic.score(x_test_scaled, y_test))

結果
0.9906103286384976
0.958041958041958

pca = PCA(n_components=2)
x_train_pca = pca.fit_transform(x_train_scaled)
print(pca.explained_variance_ratio_)

結果
[0.43315126 0.19586506]

pca_pipeline = Pipeline([
    ('scale', StandardScaler()),
    ('decomposition', PCA(n_components=2)),
    ('model', LogisticRegression())
])
pca_pipeline.fit(X_train, y_train)
print(pca_pipeline.score(X_train, y_train))
print(pca_pipeline.score(X_test, y_test))

結果
0.9624413145539906
0.9370629370629371

5. アルゴリズム
・要点：
① k近傍法
＝分類問題のための機械学習手法
説明変数の近傍のデータをk個取って、その中で一番多く所属するクラスに分類する
kの数によって精度を変えることができる
② kmeans
＝教師なし学習のクラスタリング手法
与えられたデータをk個のクラスタに分類
各クラスタ中心の初期値を設定（ランダム）→各データ点に対して、各クラスタ中心との距離を計算し、最も近いクラスタを割り当て→各クラスタの平均ベクトルを計算→収束するまで繰り返す
・ステージテスト誤答考察：
問１１　確率勾配降下法の１回目のパラメータ更新量を答えよ
間違えた理由　パラメータ更新量が定義式のマイナス以降の部分だと勘違いしていた
パラメータ更新量＝実際にパラメータに対して影響を与えた値

6. サポートベクターマシン(SVM)
・要点：
① SVM＝クラス分類アルゴリズム。決定関数のパラメータ推定方法
② マージン最大化＝マージン（分類境界を挟んだクラスの距離）を最大化し、分類精度を高める
③ 過学習を防ぐ方法＝ハードマージン（訓練データのマージンを最大化したもの）をソフトマージン（分類誤りのデータを消す）に変換する
