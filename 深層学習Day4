1.強化学習
要点：
① 強化学習＝目的（報酬）を達成（最大化）するための学習方法。方策（TT)に基づく行動によって状態（S）を変化させ、価値（V）を生む
② 強化学習と教師あり、なし学習の違い＝強化学習は優れた方策を見つけることが目標
③ Q学習＝行動価値関数を行動する毎に更新することにより学習を進める方法
④ 関数近似法＝価値関数や方策関数を関数近似する手法
⑤ 価値関数＝状態価値関数と行動価値関数の２種類ある。それぞれの関数の違いは価値の決め方が違う
⑥ 方策関数＝ある状態でどのような行動を採るのかの確率を与える関数
⑦ 方策勾配法＝適切な方策を求める方法（誤差関数は目標とする報酬）
実装演習結果：
確認テスト：
その他：

2.AlphaGO
要点：
① Alpha Go＝囲碁のAI
② Policy Net＝方策関数。最善の手を出力。計算量が多い
③ Value Net＝価値関数。勝率を-1～1の範囲で出力
④ Roll Out Policy＝ニューラルネットワークではなく線形の方策関数。探索中に高速に着手確率を出す
⑤ モンテカルロ木探索＝価値関数を更新する関数
⑥ Alpha Go Lee＝先に教師あり学習で学習してから強化学習を行う
⑦ Alpha Go Zero＝強化学習のみで学習を行う
⑧ Residiual Block＝ネットワークのショートカットを作成
実装演習結果：
確認テスト：
その他：

3.軽量化・高速化技術
要点：
① 分散深層学習＝複数の計算資源を使用し、並列的にニューラルネットワークを構成することで効率良い学習を行う
② データ並列化＝親モデルを各ワーカーに子供モデルとしてコピーし、データを分割し、各ワーカーごとに計算させる
③ モデル並列化＝親モデルを各ワーカーに分割し、それぞれのモデルを学習させ、すべてのデータの学習が終わった後に分割したモデルを統合する
④ GPU＝CPUと比較して低性能なコアを多数持つ。簡単な並列処理を得意とするため、ニューラルネットワークの学習に用いると高速化できる
⑤ 量子化＝通常のパラメータの精度を下位の精度に落とすことでメモリと演算処理を削減する
⑥ ニューラルネットワークのメモリを多く使用する理由＝重みの保管が大きい
⑦ 量子化の利点＝計算量の高速化・省メモリ化/量子化の欠点＝精度の低下
⑧ 半精度＝16bit/単精度＝32bit/倍精度＝64bit(現状半精度で問題ない）
⑨ 蒸留＝精度の高いモデルので知識を軽量なモデルへ継承させる。重みを固定する教師モデルと重みを更新できる生徒モデルを使う
⑨ プルーニング＝必要なパラメータだけを残し、不必要なパラメータをなくし計算を高速化する。重みが閾値より低いものを削減 
実装演習結果：
確認テスト：
その他：

4.応用モデル
要点：
① Mobile Net＝畳み込み演算を工夫。Depthwise Convolution（フィルタ数を１で固定し、１チャンネルごとに畳み込む）とPointwise Convolution（1*1のカーネルで畳み込む）
② Dense Net＝畳み込み演算にDenseブロックでチャネル層を増やし、Transition Layerで層をつなぎ減らす
③ Res Netとの違い＝前の層以外の情報を使うのがDense Net
④ Batch Norm＝ミニバッチに含まれるsampleの同一チャネルが同一分布に従うように正規化。計算機に合わせてデータサイズを変更しないといけないため非推奨
⑤ Layer Norm＝それぞれのsampleのすべてのpixelsが同一分布に従うよう正規化
⑥ Instance Norm＝channelも同一分布に従うように正規化
⑦ Wavenet＝音声生成モデル。時系列データに対して畳み込みを適用する
実装演習結果：
確認テスト：
その他：

5.Transformer
要点：
実装演習結果：
確認テスト：
その他：

6.物体検知・セグメンテーション
要点：
実装演習結果：
確認テスト：
その他：
