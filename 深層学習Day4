1.強化学習
要点：
① 強化学習＝目的（報酬）を達成（最大化）するための学習方法。方策（TT)に基づく行動によって状態（S）を変化させ、価値（V）を生む
② 強化学習と教師あり、なし学習の違い＝強化学習は優れた方策を見つけることが目標
③ Q学習＝行動価値関数を行動する毎に更新することにより学習を進める方法
④ 関数近似法＝価値関数や方策関数を関数近似する手法
⑤ 価値関数＝状態価値関数と行動価値関数の２種類ある。それぞれの関数の違いは価値の決め方が違う
⑥ 方策関数＝ある状態でどのような行動を採るのかの確率を与える関数
⑦ 方策勾配法＝適切な方策を求める方法（誤差関数は目標とする報酬）
実装演習結果：
確認テスト：
その他：

2.AlphaGO
要点：
① Alpha Go＝囲碁のAI
② Policy Net＝方策関数。最善の手を出力。計算量が多い
③ Value Net＝価値関数。勝率を-1～1の範囲で出力
④ Roll Out Policy＝ニューラルネットワークではなく線形の方策関数。探索中に高速に着手確率を出す
⑤ モンテカルロ木探索＝価値関数を更新する関数
⑥ Alpha Go Lee＝先に教師あり学習で学習してから強化学習を行う
⑦ Alpha Go Zero＝強化学習のみで学習を行う
⑧ Residiual Block＝ネットワークのショートカットを作成
実装演習結果：
確認テスト：
その他：

3.軽量化・高速化技術
要点：
実装演習結果：
確認テスト：
その他：

4.応用モデル
要点：
実装演習結果：
確認テスト：
その他：

5.Transformer
要点：
実装演習結果：
確認テスト：
その他：

6.物体検知・セグメンテーション
要点：
実装演習結果：
確認テスト：
その他：
