① ディープラーニングが結局何をやろうとしているのか？
A. (自分の答え）入力された値に様々な処理を加え、目的とする出力に分類すること
　（講師の答え）明示的なプログラムの代わりに多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力層に変換するを数学モデルを構築すること
② どの値の最適化が最終目的か？
A. 重み（w）とバイアス(b)
①、②の考察
① ディープラーニングは入力層の数によるが、数が増えれば増えるほどネットワークが増える。その場合、ディープラーニングによって得た出力の結果に対して、
人間が理解するのは難しい。だからこそ、講師の回答では「明示的なプログラム」という表現を使っていると考えた。
　しかし、複雑であっても入力層で入力された値から出力層を数式で導き出しており、確実な根拠があって出力をしている。だから、近年ディープラーニングへ
注目が高まっており、計算力も高まっているため実現できていると考えている。
② ディープラーニングにおいて、重みとバイアスを変更することによって出力層が変化する。①でも記載している通り、デイープラーニングがしていることは
「入力値から出力層に変換する」のため、重みとバイアスを最適化することで必要な出力値を求めることができる。だからこそ、最終目的は重みとバイアスの最適化と考えた。

③A　　　　　　　x        a
 B             y
    　　　　    z
 入力層      中間層      出力層

④

1.入力層～中間層
要点：
① 入力層＝データ及びバイアスを入力する層。入力する数には特に大きな制約はなく、入力層以降の重み等で調整することができる
② 中間層＝入力層の数値に対して重みを設定し、計算した値を次の層を出力する。中間層を増やすことで複雑なモデルを生成することができる
③ 入力層として取るべきでないデータ＝欠損値が多い、誤差が大きい、出力そのもの、出力を加工した情報、連続性のないデータ、無意味な数が割り当てられているデータ
実装演習結果：
確認テスト：
① 総入出力のpythonでの数式？
A. np.dot(x, w)+b1
② 中間層の処理の式？
A. 
u2 = np.dot(z1, W2) +b2
z2 = functions.relu(u2)
その他：

2.活性化関数
要点：
① 活性化関数＝ニューラルネットワークで入力した値を適当な形に変換する関数
② 中間層用の活性化関数＝ステップ関数、シグモイド関数（勾配消失問題発生）、ReLU関数（勾配消失関数を回避し、スパース化に貢献）
③ 出力層用の活性化関数＝ソフトマックス関数、恒等写像、シグモイド関数
実装演習結果：
確認テスト：
① 線形と非線形の違い？
A.線形＝一直線の関数。加法性、斉次性あり
非線形＝一直線ではない関数。加法性、斉次性を満たさない
② 活性化関数が使われている場所？
A. z1 = functions.relu(u1)
その他：

3.出力層
要点：
① 出力層の役割＝人間が欲しいデータの確率を出力する層
② ニューラルネットワークの学習の仕方＝訓練データを使って、出力結果と実際の回答を照合する。照合した結果の正解率によってバイアスや重みを更新
③ 出力層で誤差を計算する方法＝平均二乗和誤差、クロスエントロピー誤差（分類問題）
④ 使う活性化関数の種類　回帰＝恒等写像　二値分類＝シグモイド関数　他クラス分類＝ソフトマックス関数
実装演習結果：
確認テスト：
① なぜ引き算ではなく二乗するのか？
A. 引き算をしてしまうと＋ーの影響があるため、二乗することで＋ーの影響をなくす
② 1/2はどういう意味を持つのか？
A. 後に誤差関数を微分する際に、計算を簡単にするため
③ (1)~(3)の数式に該当するソースコードを抜き出せ(ソフトマックス関数）
(1)def softmax(x)・・・xを引数とするsoftmaxという関数を生成
(2)np.exp(x)・・・xを指数とするネイピア数を計算
(3)/np.sum(np.exp(x))・・・xを指数とするネイピア数の合計を計算
④ (1)~(2)の数式に該当するソースコードを抜き出せ（交差エントロピー）
(1)def cross_entropyy_error(d, y)・・・dとyを引数とするcross_entropyy_errorという関数を生成
(2)-np.sum(np.log(y[np.arrange(batch_size), d] + 1e-7))・・・引数yの内、指定した値の対数を合計した値
その他：

4.勾配降下法
要点：
① 種類＝確率勾配降下法、勾配降下法、ミニバッチ勾配降下法
② 勾配降下法を使用する目的＝パラメータを最適化する
③ 学習率が大きい＝最小値までいつまでたっても届かない
④ 学習率が小さい＝収束するまでの時間がかかってしまう＋極所的に低いところで学習が止まる可能性がある
⑤ ミニバッチ勾配降下法のメリット＝CPUを利用したスレッド並列化やGPUを利用したSIMD並列化
実装演習結果：
確認テスト：
① オンライン学習とは？
自分の答え　更新されていくデータを反映しながら学習を進めていく学習方法
講師の答え　学習データが入ってくるたびに都度パラメータを更新、学習を進めていく方法、一方、バッチ楽手では一度すべての学習データを使ってパラメ－タ更新を行う
その他：

5.誤差逆伝播法
要点：
① 誤差逆伝播法＝
② 計算方法・・・計算結果から微分を逆算して、不要な再帰的計算を避ける微分を算出可能
 
実装演習結果：
確認テスト：
① 既に行った計算結果を保持しているソースコードとは？
A. 
Eの微分　delta2 = function.d_mean_squared_error(d, y)
Eの微分*yの微分　delta1 = np.dot(delta2, W2.T) * fucntion.d_sigmoid(s1)
Eの微分*yの微分*uの微分　grad['W1'] = np.dot(x.T, delta1)
その他：
