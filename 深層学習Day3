1.再帰型ニューラルネットワークの概念
要点：
① RNN＝時系列データを学習ニューラルネットワーク。中間層のデータを再び中間層で読み込む
② RNNの特徴＝再帰的（前のデータを持つ）な構造を持つ
③ BPTT＝時系列データの誤差伝播関数

実装演習結果：
確認テスト：
Q.RNNの３つの重みの内、入力から現在の中間層を定義する際にかけられる重み、中間層から出力を定義する際にかけられる際の重み以外の重みについて説明せよ？
自分の答え　前の時系列データの中間層の値にかけられる重み
講師の答え　中間層から中間層への重み
Q.連鎖率の原理を使い、dz/dxを求めよ？
A.dz/dx = (dz/dt)*(dt/dx)
dz/dt = 2t
dt/dx = 1
dz/dx = 2t = 2(x + y)
Q.y1をx,s0,s1,Wi,W,Woを用いて数式で表せ？
A.s1 = f(x1*Wi + s0*W + b)
y1 = g(Wo*s1 + c)
その他：

2.LSTM
要点：
① LSTM＝RNNの勾配消失問題を構造自体を変えて解決する方法。
② 勾配爆発＝勾配が層を逆伝播することで指数関数的に大きくなっていくこと
③ CEC＝記憶するだけの機能
④ 入力ゲート＝今回の入力値と前回の出力値からCECに学ばせる
⑤ 出力ゲート＝今回の入力値と前回の出力値から今回の出力値を調整する
⑥ 忘却ゲート＝CECが持つ過去の情報を忘却する機能を持つ
実装演習結果：
確認テスト：
Q.シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいもの？
A. (2)0.25
Q.「映画面白かったね。ところで、とれてもお腹が空いたから何か...。」の...に当てはまる単語を予測するとき、どのようなゲートが作用すると考えられる？
A. 忘却ゲート  
その他：

3. GRU
要点：
① GRU＝LSTMの計算負荷を減らす方法。リセットゲームと更新ゲートを使う
② リセットゲート＝隠れ層の
③ 更新ゲート＝前回の記憶と今回の記憶の活用割合を調整

実装演習結果：
確認テスト：
① LSTNとCECが抱える課題とは？
自分の答え　LSTM＝学習不可が高くなる/CEC＝記憶の可能
講師の答え　LSTM＝計算量が大きくなる/CEC＝学習機能がない
②　LSTMとGRMの違いを述べよ？
自分の答え　GRUの方がゲート数が少ないため、計算負荷が下がる
講師の答え　パラメータがGRMの方が少ないため、計算量が少ない
その他：

4.双方向RNN
要点： 
① 双方向RNN＝過去の情報だけでなく、未来の情報を加味したモデル
② 合成方法＝axis１で合成する

実装演習結果：
確認テスト：
その他：

5.Seq2Seq
要点：
① Seq2Seq＝機械翻訳に使われるニューラルネットワーク。エンコーダ、デコーダモデルの一種
② エンコーダ＝データをトークンに区切って渡す構造
③ デコーダ＝アウトプットデータをトークンごとに生成する構造
④ MLM＝与えられたデータの一部分を落とし、その部分に入れるべきベクトルを学習
実装演習結果：
確認テスト：
Q.seq2seqについて説明しているものを述べよ？
A.(2)RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる
その他：

6.Word2vec 
要点：
① Word2vec＝単語や文字列をベクトル表現にする手法
② 構造　文章で使われている単語をリストアップ→単語に番号を振る→単語をベクトル化する
③ 変換表を作成する
実装演習結果：
確認テスト：
その他：

7.Attention Mechanism
要点：
① Attention Mechanism＝一文の中で重要な単語を見分けて抜き出す方法
② 長文に強い。
実装演習結果：
確認テスト：
Q.RNN、word2vec、seq2seq、Attentionの違い？
A.RNN＝時系列データ
word2vec＝単語の分散表現を得る手法
seq2seq＝１つの時系列データから他の時系列データを出力する方法
Attention＝１つの時系列データの関連性に重みを作る手法
その他：
