1.再帰型ニューラルネットワークの概念
要点：
① 再帰型ニューラルネットワーク＝RNN（Recurrent Neural Network)
② RNN＝時系列データを学習ニューラルネットワーク。中間層のデータを再び中間層で読み込む
③ RNNの特徴＝再帰的（前のデータを持つ）な構造を持つ
④ BPTT＝時系列データの誤差伝播関数
確認テスト：
Q.RNNの３つの重みの内、入力から現在の中間層を定義する際にかけられる重み、中間層から出力を定義する際にかけられる際の重み以外の重みについて説明せよ？
自分の答え　前の時系列データの中間層の値にかけられる重み
講師の答え　中間層から中間層への重み
Q.連鎖率の原理を使い、dz/dxを求めよ？
A.dz/dx = (dz/dt)*(dt/dx)
dz/dt = 2t
dt/dx = 1
dz/dx = 2t = 2(x + y)
Q.y1をx,s0,s1,Wi,W,Woを用いて数式で表せ？
A.s1 = f(x1*Wi + s0*W + b)
y1 = g(Wo*s1 + c)
実装演習結果：
# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#　通常結果（結果が長くなりすぎるため、plot_intervalを2000に変更
iters:0
Loss:1.4644245418863542
Pred:[0 1 0 1 1 1 1 1]
True:[1 0 0 0 1 0 1 0]
94 + 44 = 95
------------
iters:2000
Loss:0.8694590904992542
Pred:[0 1 1 1 0 0 0 0]
True:[0 1 1 1 0 1 1 0]
26 + 92 = 112
------------
iters:4000
Loss:1.3385279788551725
Pred:[1 0 0 1 1 1 0 0]
True:[1 0 1 0 0 0 0 0]
99 + 61 = 156
------------
iters:6000
Loss:0.06379397058095292
Pred:[1 1 0 0 0 1 1 1]
True:[1 1 0 0 0 1 1 1]
124 + 75 = 199
------------
iters:8000
Loss:0.010394733846150705
Pred:[1 0 1 1 1 0 1 0]
True:[1 0 1 1 1 0 1 0]
98 + 88 = 186
------------
# ## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう
# weight_init_std を0.5に変更
iters:0
Loss:1.026514938090001
Pred:[0 0 0 0 0 0 0 0]
True:[1 1 0 0 1 0 0 0]
107 + 93 = 0
------------
iters:2000
Loss:0.9754001983845508
Pred:[1 1 1 1 1 1 0 0]
True:[1 0 0 1 0 1 0 0]
108 + 40 = 252
------------
iters:4000
Loss:0.6654014667670012
Pred:[0 0 0 0 1 1 0 1]
True:[0 1 0 0 1 1 0 1]
73 + 4 = 13
------------
iters:6000
Loss:0.1643968363605778
Pred:[0 1 0 1 1 0 0 1]
True:[0 1 0 1 1 0 0 1]
37 + 52 = 89
------------
iters:8000
Loss:0.06688487658229174
Pred:[1 0 0 1 1 1 1 1]
True:[1 0 0 1 1 1 1 1]
78 + 81 = 159
------------
# learning_rateを0.2に変更
iters:0
Loss:1.8847274333451276
Pred:[1 1 1 1 1 1 1 1]
True:[0 1 0 1 0 1 0 0]
25 + 59 = 255
------------
iters:2000
Loss:0.604054401903852
Pred:[1 0 0 0 1 1 1 1]
True:[1 0 1 0 1 1 1 1]
61 + 114 = 143
------------
iters:4000
Loss:0.02021686913557642
Pred:[0 1 0 0 1 1 0 1]
True:[0 1 0 0 1 1 0 1]
16 + 61 = 77
------------
iters:6000
Loss:0.0030024925725943823
Pred:[0 0 1 1 1 1 0 0]
True:[0 0 1 1 1 1 0 0]
35 + 25 = 60
------------
iters:8000
Loss:0.0009283751801022031
Pred:[0 1 0 1 1 0 1 1]
True:[0 1 0 1 1 0 1 1]
63 + 28 = 91
------------
# hidden_layer_sizeを8に変更
iters:0
Loss:1.6461056162720915
Pred:[0 0 0 0 0 0 0 0]
True:[1 1 1 0 1 0 0 0]
120 + 112 = 0
------------
iters:2000
Loss:0.9308669935472332
Pred:[1 0 1 1 1 0 1 0]
True:[1 0 1 1 1 1 0 0]
91 + 97 = 186
------------
iters:4000
Loss:0.7777353867084824
Pred:[1 1 1 0 1 1 0 0]
True:[1 0 1 0 1 0 0 1]
114 + 55 = 236
------------
iters:6000
Loss:0.07226612050273296
Pred:[0 1 1 1 1 0 0 0]
True:[0 1 1 1 1 0 0 0]
19 + 101 = 120
------------
iters:8000
Loss:0.08943096478719574
Pred:[1 0 0 1 0 0 0 0]
True:[1 0 0 1 0 0 0 0]
54 + 90 = 144
------------
# 
# 
# ## [try] 重みの初期化方法を変更してみよう
# Xavier
iters:0
Loss:1.5679054257231766
Pred:[1 1 1 1 1 1 1 1]
True:[0 1 0 1 0 0 0 0]
25 + 55 = 255
------------
iters:2000
Loss:0.9885890621491252
Pred:[0 0 0 0 0 0 0 0]
True:[1 0 0 1 0 1 1 0]
100 + 50 = 0
------------
iters:4000
Loss:0.9645878805289262
Pred:[1 1 0 1 0 1 0 0]
True:[1 0 0 1 1 0 0 0]
106 + 46 = 212
------------
iters:6000
Loss:0.5677214553528738
Pred:[1 0 1 1 1 0 1 0]
True:[0 1 1 1 1 0 1 0]
18 + 104 = 186
------------
iters:8000
Loss:0.021275078482933647
Pred:[0 1 1 1 1 1 0 1]
True:[0 1 1 1 1 1 0 1]
120 + 5 = 125
------------
# He
iters:0
Loss:0.838404049722835
Pred:[0 0 0 0 0 0 0 0]
True:[0 1 1 0 0 0 0 0]
34 + 62 = 0
------------
iters:2000
Loss:0.9027931990175398
Pred:[0 1 1 0 1 0 1 1]
True:[0 1 0 0 1 1 1 1]
27 + 52 = 107
------------
iters:4000
Loss:0.4779136073396952
Pred:[0 1 0 1 1 1 0 0]
True:[0 1 0 1 1 1 0 0]
17 + 75 = 92
------------
iters:6000
Loss:0.026077405847868485
Pred:[0 0 1 1 0 0 1 1]
True:[0 0 1 1 0 0 1 1]
10 + 41 = 51
------------
iters:8000
Loss:0.004960368771545
Pred:[1 0 0 1 1 0 1 1]
True:[1 0 0 1 1 0 1 1]
61 + 94 = 155
------------
# 
# ## [try] 中間層の活性化関数を変更してみよう
# ReLU(勾配爆発を確認しよう)<br>
iters:0
Loss:1.004287140398238
Pred:[1 1 1 1 1 1 1 1]
True:[1 0 1 1 1 0 1 1]
83 + 104 = 255
------------
RuntimeWarning: overflow encountered in exp
  return 1/(1 + np.exp(-x))
ValueError: cannot convert float NaN to integer
# tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)
iters:0
Loss:1.592867301513008
Pred:[1 0 1 1 1 0 0 1]
True:[1 0 0 0 1 1 1 0]
101 + 41 = 185
------------
iters:2000
Loss:1.0227777002603111
Pred:[1 1 0 0 0 0 0 0]
True:[1 0 1 0 0 0 0 0]
40 + 120 = 192
------------
iters:4000
Loss:0.8771103328932945
Pred:[1 0 0 1 1 0 1 0]
True:[1 1 0 1 1 1 0 0]
95 + 125 = 154
------------
iters:6000
Loss:0.6181521265991986
Pred:[1 0 0 0 1 0 0 1]
True:[1 0 0 1 1 0 0 1]
44 + 109 = 137
------------
iters:8000
Loss:1.053844080916569
Pred:[1 1 0 0 0 1 1 0]
True:[0 1 1 1 0 0 1 0]
66 + 48 = 198
------------
# 
# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


2.LSTM
要点：
① LSTM＝RNNの勾配消失問題を構造自体を変えて解決する方法。
② 勾配爆発＝勾配が層を逆伝播することで指数関数的に大きくなっていくこと
③ CEC＝記憶するだけの機能
④ 入力ゲート＝今回の入力値と前回の出力値からCECに学ばせる
⑤ 出力ゲート＝今回の入力値と前回の出力値から今回の出力値を調整する
⑥ 忘却ゲート＝CECが持つ過去の情報を忘却する機能を持つ
確認テスト：
Q.シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいもの？
A. (2)0.25
Q.「映画面白かったね。ところで、とれてもお腹が空いたから何か...。」の...に当てはまる単語を予測するとき、どのようなゲートが作用すると考えられる？
A. 忘却ゲート  

3. GRU
要点：
① GRU(Gated recurrent unit)＝LSTMの計算負荷が高いため、それを減らすために編み出された方法。リセットゲームと更新ゲートを使う
② リセットゲート＝過去の情報をいかに反映するのかを決定
③ 更新ゲート＝前回の記憶と今回の記憶の活用割合を調整
確認テスト：
① LSTNとCECが抱える課題とは？
自分の答え　LSTM＝学習不可が高くなる/CEC＝記憶の可能
講師の答え　LSTM＝計算量が大きくなる/CEC＝学習機能がない
②　LSTMとGRMの違いを述べよ？
自分の答え　GRUの方がゲート数が少ないため、計算負荷が下がる
講師の答え　パラメータがGRMの方が少ないため、計算量が少ない
実装演習結果：
# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# 通常
loss: 5.111112056304763e-07    d: [-0.47157024]    y: [-0.47258129]
loss: 1.4153640452728792e-06    d: [-0.39789889]    y: [-0.39958137]
loss: 2.559902991488718e-06    d: [-0.78740743]    y: [-0.78967013]
loss: 7.216263290783507e-09    d: [0.25526991]    y: [0.25514977]
loss: 1.692063852303632e-06    d: [0.6529121]    y: [0.6547517]
loss: 7.238070929589735e-07    d: [0.8773359]    y: [0.87613273]
loss: 6.94623672640184e-07    d: [0.92114593]    y: [0.91996727]
loss: 1.2284734878711618e-06    d: [-0.58880346]    y: [-0.59037093]
loss: 1.249746554038834e-08    d: [-0.6529121]    y: [-0.65307019]
loss: 1.747012227351786e-06    d: [-0.32751865]    y: [-0.32938788]
loss: 2.0214401036935557e-07    d: [0.39789889]    y: [0.39853473]
loss: 1.7248378220182004e-06    d: [-0.15674537]    y: [-0.1586027]
loss: 1.3462666565758106e-06    d: [0.97076771]    y: [0.97240861]
loss: 6.884294927342299e-07    d: [-0.50453668]    y: [-0.50571008]
loss: 8.964485145726196e-09    d: [-0.99460929]    y: [-0.99447539]
loss: 1.6819644091700236e-06    d: [0.95745284]    y: [0.95928694]
loss: 6.636708769141727e-07    d: [-0.92833248]    y: [-0.92718037]
loss: 1.6550737476982935e-06    d: [-0.35120641]    y: [-0.35302579]
loss: 1.4812813327813093e-06    d: [-0.3863158]    y: [-0.38803701]
loss: 1.5504762308512516e-06    d: [0.63363256]    y: [0.63539351]
loss: 1.0859688992783592e-06    d: [0.44921588]    y: [0.45068963]
loss: 7.024336524123257e-07    d: [-0.86811636]    y: [-0.86693109]
loss: 4.298419312275074e-07    d: [0.45483173]    y: [0.45575893]
loss: 1.5800953987150462e-07    d: [0.59893397]    y: [0.59949613]
loss: 1.305987382511987e-07    d: [0.18156486]    y: [0.18105379]
loss: 1.6215636803484033e-06    d: [0.64332332]    y: [0.64512419]
loss: 2.6565935995811714e-08    d: [0.99247351]    y: [0.99224301]
loss: 7.407414549659186e-07    d: [-0.89762559]    y: [-0.89640842]
loss: 3.7108253202912373e-07    d: [-0.11308158]    y: [-0.11222009]
loss: 2.5763917247784403e-06    d: [0.79127273]    y: [0.7935427]
loss: 1.656899449754742e-06    d: [-0.64813056]    y: [-0.64995094]
loss: 4.637650286011886e-07    d: [0.95561698]    y: [0.9546539]
loss: 3.6393779690636794e-07    d: [0.99690497]    y: [0.99775813]
loss: 2.2160406235611887e-07    d: [-0.4036669]    y: [-0.40433264]
loss: 5.740145938550149e-08    d: [-0.99987614]    y: [-1.00021496]
loss: 2.762903013775288e-07    d: [-0.57343317]    y: [-0.57417653]
loss: 1.855495428968903e-07    d: [0.75537465]    y: [0.75476547]
loss: 9.152860658438427e-07    d: [0.00629574]    y: [0.00494275]
loss: 1.8797284550728012e-06    d: [-0.23084276]    y: [-0.23278169]
loss: 1.0524302613338606e-06    d: [0.56307233]    y: [0.56452314]
loss: 4.631459840874837e-08    d: [0.99975723]    y: [1.00006158]
loss: 3.447189422141964e-07    d: [0.11933469]    y: [0.11850437]
loss: 1.883099480769221e-06    d: [0.23696388]    y: [0.23890455]
loss: 5.1214730714751955e-09    d: [0.99524241]    y: [0.9951412]
loss: 1.051230447773728e-06    d: [-0.98039956]    y: [-0.98184954]
loss: 1.813882872732977e-06    d: [0.18775236]    y: [0.18965703]
loss: 5.27036239975623e-07    d: [0.82463104]    y: [0.82360436]
loss: 5.169924703573374e-07    d: [-0.94988243]    y: [-0.94886558]
loss: 6.747741177793689e-07    d: [0.92597363]    y: [0.92481193]
loss: 1.8809353374298544e-06    d: [0.26135201]    y: [0.26329156]
loss: 2.0325852791579403e-07    d: [-0.75948523]    y: [-0.75884764]
loss: 9.92544090722427e-08    d: [-0.98611478]    y: [-0.98566924]
loss: 3.54176004989751e-07    d: [-0.43793098]    y: [-0.43877262]
loss: 3.137779621326153e-08    d: [0.23084276]    y: [0.23059225]
loss: 2.6937112046164813e-06    d: [0.83516734]    y: [0.83748842]
loss: 2.588227073819169e-07    d: [0.77163571]    y: [0.77091623]
loss: 1.2613599482853231e-06    d: [0.97371292]    y: [0.97530122]
loss: 1.4907373517710847e-07    d: [-0.99975723]    y: [-1.00030326]
loss: 4.232044587656063e-07    d: [0.99583607]    y: [0.99675608]
loss: 3.787447800805401e-07    d: [0.44358222]    y: [0.44445256]
loss: 1.8816335271670955e-06    d: [-0.94789551]    y: [-0.94983543]
loss: 1.2190012629922352e-06    d: [-0.97512765]    y: [-0.97668906]
loss: 1.996710038100712e-06    d: [-0.694759]    y: [-0.69675736]
loss: 1.8849134192596554e-06    d: [-0.2430756]    y: [-0.24501721]
loss: 1.8748123442560268e-06    d: [0.22471249]    y: [0.22664888]
loss: 4.2620357543512357e-07    d: [-0.10056216]    y: [-0.0996389]
loss: 4.325800120889982e-09    d: [0.68564779]    y: [0.68555477]
loss: 1.8306176723615103e-06    d: [0.29761864]    y: [0.29953208]
loss: 2.332044298284346e-09    d: [-0.99583607]    y: [-0.99576778]
loss: 6.585761031379055e-07    d: [-0.99085292]    y: [-0.99200059]
loss: 7.651581590766538e-08    d: [0.35120641]    y: [0.3515976]
loss: 7.290859282135365e-07    d: [-0.88033969]    y: [-0.87913214]
loss: 1.6685814037161896e-09    d: [-0.68105132]    y: [-0.68099355]
loss: 2.691838713149061e-06    d: [0.85534252]    y: [0.85766279]
loss: 7.327100876992387e-07    d: [-0.98907524]    y: [-0.99028578]
loss: 5.4908929482703205e-08    d: [-0.71705202]    y: [-0.71672063]
loss: 6.837326417292688e-07    d: [-0.86179776]    y: [-0.86062837]
loss: 1.6566551435475068e-06    d: [0.13806466]    y: [0.13988491]
loss: 5.681550927763364e-07    d: [-0.48263615]    y: [-0.48370213]
loss: 2.2589028031750398e-07    d: [-0.15052435]    y: [-0.1498522]
loss: 1.7452123204601994e-06    d: [0.16296018]    y: [0.16482845]
loss: 2.09155034432836e-06    d: [0.70821885]    y: [0.71026411]
loss: 1.7631822152185102e-06    d: [0.95374324]    y: [0.9556211]
loss: 2.4114224765198005e-07    d: [0.97512765]    y: [0.97443319]
loss: 2.067490890143368e-06    d: [0.93739898]    y: [0.93943244]
loss: 8.485394806789291e-07    d: [0.98611478]    y: [0.9874175]
loss: 1.72986325908338e-07    d: [0.98039956]    y: [0.97981136]
loss: 3.9139136578767244e-07    d: [-0.55262221]    y: [-0.55350696]
loss: 5.144245939051554e-07    d: [0.08175375]    y: [0.08073943]
loss: 1.5434925122990647e-06    d: [-0.37467145]    y: [-0.37642843]
loss: 8.64601116962286e-08    d: [0.98714074]    y: [0.9867249]
loss: 1.7821870821634795e-06    d: [0.17537017]    y: [0.17725812]
loss: 1.7227716039716527e-06    d: [-0.95561698]    y: [-0.9574732]
loss: 1.7032584357931716e-06    d: [0.15052435]    y: [0.15237003]
loss: 2.2961399277292755e-06    d: [-0.92114593]    y: [-0.92328889]
loss: 1.4861572178323642e-07    d: [-0.38050117]    y: [-0.38104635]
loss: 8.149646046184465e-07    d: [-0.48814053]    y: [-0.48941721]
loss: 9.154701847834834e-07    d: [0.54208448]    y: [0.5434376]
loss: 8.19187220025201e-09    d: [-0.69021707]    y: [-0.69008907]
loss: 1.1946819048400479e-07    d: [0.60896952]    y: [0.60945833]
# ### [try]
# -  iters_numを100にしよう
loss: 3.757482812633308e-06    d: [-0.29761864]    y: [-0.2948773]
loss: 1.0719859191917618e-05    d: [-0.56307233]    y: [-0.55844202]
loss: 9.952296164318537e-07    d: [-0.65766776]    y: [-0.65625692]
loss: 5.5732072876682295e-06    d: [0.13182648]    y: [0.12848785]
loss: 1.4019829836170735e-06    d: [0.49909101]    y: [0.4974165]
loss: 8.159677580999151e-05    d: [0.9518317]    y: [0.93905698]
loss: 7.957987428129466e-05    d: [0.97784112]    y: [0.96522527]
loss: 9.80279679947469e-07    d: [-0.58880346]    y: [-0.58740326]
loss: 4.941681044532561e-05    d: [-0.78351093]    y: [-0.77356942]
loss: 5.08222655072845e-06    d: [-0.49909101]    y: [-0.49590283]
loss: 4.805052837245578e-06    d: [0.21857331]    y: [0.21547329]
loss: 4.47857511706312e-09    d: [-0.33938943]    y: [-0.33929479]
loss: 1.9457806349626948e-06    d: [-0.43793098]    y: [-0.43595828]
loss: 3.253356266480612e-06    d: [-0.33346065]    y: [-0.33090982]
loss: 4.97927654221475e-05    d: [-0.99639027]    y: [-0.98641101]
loss: 9.019819110325985e-06    d: [0.88624247]    y: [0.88199516]
loss: 7.980890434089906e-05    d: [-0.92833248]    y: [-0.91569848]
loss: 6.719253434023182e-06    d: [-0.52075286]    y: [-0.517087]
loss: 9.629997700230058e-06    d: [-0.55262221]    y: [-0.54823359]
loss: 1.5745019912690713e-06    d: [0.47711265]    y: [0.47533811]
loss: 1.6329079981200305e-05    d: [0.60896952]    y: [0.60325479]
loss: 8.136681520151185e-05    d: [-0.94587102]    y: [-0.93311432]
loss: 4.010554429741591e-06    d: [0.27953518]    y: [0.27670303]
loss: 3.937679513342539e-05    d: [0.73863456]    y: [0.72976024]
loss: 5.239594090967559e-06    d: [-0.00629574]    y: [-0.00953289]
loss: 1.1423543358824378e-06    d: [0.54208448]    y: [0.54057295]
loss: 5.226351522437884e-05    d: [0.99781582]    y: [0.98759197]
loss: 8.137848289019104e-05    d: [-0.96441607]    y: [-0.95165845]
loss: 4.259421379074909e-06    d: [0.07547747]    y: [0.07839618]
loss: 1.0096746752976323e-06    d: [0.66239735]    y: [0.66097632]
loss: 1.1176922527937846e-06    d: [-0.54736419]    y: [-0.54586907]
loss: 7.256949086940453e-05    d: [0.99393675]    y: [0.98188939]
loss: 2.695223707461198e-05    d: [0.96441607]    y: [0.9570741]
loss: 4.731551232380385e-06    d: [-0.22471249]    y: [-0.22163627]
loss: 4.648344783916632e-05    d: [-0.99393675]    y: [-0.98429482]
loss: 3.4858159593043436e-05    d: [-0.71705202]    y: [-0.70870239]
loss: 6.830227500060832e-05    d: [0.8649742]    y: [0.85328641]
loss: 3.422400233534747e-06    d: [-0.11933469]    y: [-0.12195095]
loss: 9.106196887542645e-07    d: [-0.40941891]    y: [-0.40806937]
loss: 2.4013392621906664e-06    d: [0.39789889]    y: [0.39570739]
loss: 3.9107235899330035e-05    d: [0.98611478]    y: [0.97727088]
loss: 4.367619875321698e-06    d: [-0.0691982]    y: [-0.07215375]
loss: 7.820922024006318e-08    d: [0.35709413]    y: [0.35669864]
loss: 4.896616405596609e-05    d: [0.99583607]    y: [0.98594]
loss: 1.5412299963576907e-05    d: [-0.92597363]    y: [-0.92042164]
loss: 1.8067162455868827e-07    d: [0.36882689]    y: [0.36822577]
loss: 7.815404147783694e-05    d: [0.91617219]    y: [0.90366986]
loss: 7.1665767464166964e-06    d: [0.52611726]    y: [0.52233134]
loss: 8.126175388972179e-05    d: [0.96606148]    y: [0.95331301]
loss: 1.820078715496743e-06    d: [0.43793098]    y: [0.43602306]
loss: 6.899088023056186e-05    d: [-0.86811636]    y: [-0.8563698]
loss: 5.791801880666163e-05    d: [-0.99975723]    y: [-0.98899452]
loss: 2.334600633107689e-06    d: [0.77562491]    y: [0.77346408]
loss: 5.618337628795266e-06    d: [0.04405617]    y: [0.04070406]
loss: 1.3579455280501299e-06    d: [0.71705202]    y: [0.71540403]
loss: 7.096679990964392e-05    d: [0.8773359]    y: [0.86542231]
loss: 1.30118941207464e-05    d: [0.91363079]    y: [0.90852944]
loss: 3.358245300675579e-05    d: [-0.97784112]    y: [-0.9696457]
loss: 2.556909556715172e-05    d: [0.96101064]    y: [0.95385954]
loss: 4.177340366397119e-06    d: [0.26742375]    y: [0.2645333]
loss: 7.410381673195005e-06    d: [-0.87122411]    y: [-0.86737434]
loss: 1.3470123318259226e-05    d: [-0.91617219]    y: [-0.91098179]
loss: 6.966476359396802e-05    d: [0.87122411]    y: [0.85942032]
loss: 3.750311380401813e-05    d: [0.98394564]    y: [0.97528503]
loss: 7.688997170537062e-07    d: [0.4036669]    y: [0.40242682]
loss: 4.033518716169829e-06    d: [0.0880268]    y: [0.09086705]
loss: 5.563406469051993e-05    d: [0.81012572]    y: [0.79957735]
loss: 1.0654530954284697e-06    d: [0.41515469]    y: [0.41369492]
loss: 4.813878826545959e-05    d: [-0.99524241]    y: [-0.98543029]
loss: 2.1025998626649902e-05    d: [-0.94789551]    y: [-0.94141076]
loss: 5.313829156155314e-06    d: [0.16916853]    y: [0.16590852]
loss: 8.163019241746096e-05    d: [-0.95374324]    y: [-0.9409659]
loss: 3.665538761792845e-05    d: [-0.72577151]    y: [-0.71720934]
loss: 1.683867500491897e-06    d: [0.74286391]    y: [0.74102877]
loss: 1.9818235758789805e-05    d: [-0.94380904]    y: [-0.93751329]
loss: 6.151140287860853e-05    d: [-0.83516734]    y: [-0.82407577]
loss: 8.110557828782796e-05    d: [-0.94170965]    y: [-0.92897344]
loss: 1.9318796643800524e-08    d: [0.32156366]    y: [0.32176023]
loss: 1.5286611824297058e-06    d: [-0.48263615]    y: [-0.48088763]
loss: 4.854576227064308e-06    d: [0.03776568]    y: [0.04088163]
loss: 1.879586593745062e-08    d: [0.34530476]    y: [0.34511088]
loss: 1.0537888202671884e-06    d: [0.56307233]    y: [0.56162058]
loss: 1.2128061636036374e-05    d: [0.90843947]    y: [0.90351442]
loss: 6.47518142624273e-05    d: [0.99940055]    y: [0.98802059]
loss: 6.039549170371373e-06    d: [0.85534252]    y: [0.85186702]
loss: 1.8085375355119298e-05    d: [0.93739898]    y: [0.93138477]
loss: 6.888474314269987e-05    d: [0.99738016]    y: [0.98564264]
loss: 3.132140255418337e-05    d: [-0.6992734]    y: [-0.69135868]
loss: 3.6737500038945963e-06    d: [-0.10682399]    y: [-0.10953462]
loss: 8.599503641645753e-06    d: [-0.54208448]    y: [-0.53793731]
loss: 5.712381364890678e-05    d: [0.9995987]    y: [0.98891004]
loss: 1.617633440593614e-07    d: [0.29761864]    y: [0.29818744]
loss: 4.565633881742255e-05    d: [0.99322482]    y: [0.98366905]
loss: 6.227315466762706e-12    d: [0.33346065]    y: [0.33346418]
loss: 4.485513045337468e-06    d: [-0.83168816]    y: [-0.82869299]
loss: 3.8302957328799215e-05    d: [-0.98504973]    y: [-0.97629725]
loss: 2.1420768479425394e-05    d: [-0.64332332]    y: [-0.63677798]
loss: 2.005891007980748e-06    d: [0.43226238]    y: [0.43025944]
loss: 5.6498478338830656e-05    d: [-0.81380058]    y: [-0.80317058]
# -  maxlenを5, iters_numを500, 3000(※時間がかかる)にしよう
# iters_num 500
loss: 1.910653011300022e-06    d: [-0.29761864]    y: [-0.29566383]
loss: 4.009145134728017e-06    d: [-0.56307233]    y: [-0.56024067]
loss: 4.117004181531573e-06    d: [-0.65766776]    y: [-0.65479826]
loss: 1.2186275784356574e-06    d: [0.13182648]    y: [0.13026531]
loss: 2.8569091862194563e-06    d: [0.49909101]    y: [0.49670065]
loss: 2.968354581639092e-05    d: [0.9518317]    y: [0.9441267]
loss: 3.0276143721348603e-05    d: [0.97784112]    y: [0.97005959]
loss: 3.4625189230101373e-06    d: [-0.58880346]    y: [-0.58617192]
loss: 1.6259323267393e-05    d: [-0.78351093]    y: [-0.77780842]
loss: 2.265698201957433e-06    d: [-0.49909101]    y: [-0.4969623]
loss: 1.5821431059884354e-06    d: [0.21857331]    y: [0.21679447]
loss: 2.3430828768757427e-07    d: [-0.33938943]    y: [-0.33870488]
loss: 2.533779797625671e-06    d: [-0.43793098]    y: [-0.43567986]
loss: 2.0617518493141917e-06    d: [-0.33346065]    y: [-0.33143001]
loss: 2.3604686128421856e-05    d: [-0.99639027]    y: [-0.98951936]
loss: 1.0034138848189659e-05    d: [0.88624247]    y: [0.8817627]
loss: 2.828232109583138e-05    d: [-0.92833248]    y: [-0.92081153]
loss: 2.781847442423939e-06    d: [-0.52075286]    y: [-0.51839411]
loss: 3.6775470081978387e-06    d: [-0.55262221]    y: [-0.54991018]
loss: 2.734801351891209e-06    d: [0.47711265]    y: [0.47477393]
loss: 5.707001416447295e-06    d: [0.60896952]    y: [0.60559106]
loss: 2.9379645845479965e-05    d: [-0.94587102]    y: [-0.93820556]
loss: 1.835241060704508e-06    d: [0.27953518]    y: [0.27761933]
loss: 1.290939972426371e-05    d: [0.73863456]    y: [0.73355334]
loss: 6.36412499171634e-07    d: [-0.00629574]    y: [-0.00742393]
loss: 3.121808149452584e-06    d: [0.54208448]    y: [0.53958575]
loss: 2.4277502169551396e-05    d: [0.99781582]    y: [0.99084767]
loss: 3.0156535739503847e-05    d: [-0.96441607]    y: [-0.95664992]
loss: 3.6644646838176887e-07    d: [0.07547747]    y: [0.07633357]
loss: 4.170942068393715e-06    d: [0.66239735]    y: [0.65950912]
loss: 3.157202319311419e-06    d: [-0.54736419]    y: [-0.54485134]
loss: 2.920107362630238e-05    d: [0.99393675]    y: [0.98629462]
loss: 1.6777427734815166e-05    d: [0.96441607]    y: [0.95862342]
loss: 1.6076515357790614e-06    d: [-0.22471249]    y: [-0.22291936]
loss: 2.268448168700773e-05    d: [-0.99393675]    y: [-0.9872011]
loss: 1.1450491975764488e-05    d: [-0.71705202]    y: [-0.71226653]
loss: 2.313493553559152e-05    d: [0.8649742]    y: [0.85817201]
loss: 2.1759433323158786e-07    d: [-0.11933469]    y: [-0.11999438]
loss: 7.988432737297375e-07    d: [-0.40941891]    y: [-0.40815491]
loss: 2.3447095198498435e-06    d: [0.39789889]    y: [0.39573339]
loss: 2.0555094439676174e-05    d: [0.98611478]    y: [0.97970306]
loss: 3.8950406734359047e-07    d: [-0.0691982]    y: [-0.07008082]
loss: 3.389793739118198e-07    d: [0.35709413]    y: [0.35627075]
loss: 2.3376863487894483e-05    d: [0.99583607]    y: [0.9889984]
loss: 1.2711586427779869e-05    d: [-0.92597363]    y: [-0.92093149]
loss: 4.2154733560528485e-07    d: [0.36882689]    y: [0.36790869]
loss: 2.7395603780695072e-05    d: [0.91617219]    y: [0.90877008]
loss: 2.9209050974610185e-06    d: [0.52611726]    y: [0.52370027]
loss: 3.019689599675032e-05    d: [0.96606148]    y: [0.95829014]
loss: 1.160844979033234e-06    d: [0.43793098]    y: [0.43640727]
loss: 2.340770840482707e-05    d: [-0.86811636]    y: [-0.86127418]
loss: 2.576901643919377e-05    d: [-0.99975723]    y: [-0.99257823]
loss: 6.035275092433081e-06    d: [0.77562491]    y: [0.77215064]
loss: 8.464031632310867e-07    d: [0.04405617]    y: [0.04275509]
loss: 4.910881796040187e-06    d: [0.71705202]    y: [0.71391805]
loss: 2.4204467179015956e-05    d: [0.8773359]    y: [0.87037824]
loss: 1.1761529331284156e-05    d: [0.91363079]    y: [0.90878074]
loss: 1.888448085857942e-05    d: [-0.97784112]    y: [-0.97169547]
loss: 1.6321019461936906e-05    d: [0.96101064]    y: [0.95529732]
loss: 1.7848998612739413e-06    d: [0.26742375]    y: [0.26553435]
loss: 9.258739787364421e-06    d: [-0.87122411]    y: [-0.86692092]
loss: 1.1947044111106726e-05    d: [-0.91617219]    y: [-0.91128403]
loss: 2.367698734862928e-05    d: [0.87122411]    y: [0.86434269]
loss: 2.0077118853706327e-05    d: [0.98394564]    y: [0.97760891]
loss: 7.360472484870854e-07    d: [0.4036669]    y: [0.4024536]
loss: 3.215348705019612e-07    d: [0.0880268]    y: [0.08882871]
loss: 1.8423057856320083e-05    d: [0.81012572]    y: [0.80405562]
loss: 8.647639137492504e-07    d: [0.41515469]    y: [0.41383957]
loss: 2.314747322926963e-05    d: [-0.99524241]    y: [-0.98843837]
loss: 1.4770673004443205e-05    d: [-0.94789551]    y: [-0.94246032]
loss: 1.3759144988001705e-06    d: [0.16916853]    y: [0.16750966]
loss: 2.977144803366257e-05    d: [-0.95374324]    y: [-0.94602684]
loss: 1.2027467468923719e-05    d: [-0.72577151]    y: [-0.72086693]
loss: 5.3543178256849366e-06    d: [0.74286391]    y: [0.73959151]
loss: 1.4343081452792383e-05    d: [-0.94380904]    y: [-0.9384531]
loss: 2.0548253408385015e-05    d: [-0.83516734]    y: [-0.82875668]
loss: 2.9143926986217627e-05    d: [-0.94170965]    y: [-0.934075]
loss: 1.5117298951897392e-07    d: [0.32156366]    y: [0.3210138]
loss: 2.7647736883463247e-06    d: [-0.48263615]    y: [-0.48028466]
loss: 5.098030445281328e-07    d: [0.03776568]    y: [0.03877543]
loss: 2.667236115176528e-07    d: [0.34530476]    y: [0.34457439]
loss: 3.2668780046007232e-06    d: [0.56307233]    y: [0.56051621]
loss: 1.1397375708728137e-05    d: [0.90843947]    y: [0.90366509]
loss: 2.7467117229236253e-05    d: [0.99940055]    y: [0.99198879]
loss: 8.541817963324537e-06    d: [0.85534252]    y: [0.85120928]
loss: 1.3715986197113619e-05    d: [0.93739898]    y: [0.93216142]
loss: 2.8420473380281174e-05    d: [0.99738016]    y: [0.98984087]
loss: 1.0326987536999018e-05    d: [-0.6992734]    y: [-0.69472873]
loss: 2.575927071064799e-07    d: [-0.10682399]    y: [-0.10754175]
loss: 3.3625419133653303e-06    d: [-0.54208448]    y: [-0.5394912]
loss: 2.5563812242314734e-05    d: [0.9995987]    y: [0.99244835]
loss: 7.097181964460716e-08    d: [0.29761864]    y: [0.29724189]
loss: 2.245112358569499e-05    d: [0.99322482]    y: [0.9865239]
loss: 2.0428571992726324e-07    d: [0.33346065]    y: [0.33282145]
loss: 7.633696195162358e-06    d: [-0.83168816]    y: [-0.82778081]
loss: 2.0316142326328007e-05    d: [-0.98504973]    y: [-0.97867539]
loss: 7.255100230457174e-06    d: [-0.64332332]    y: [-0.6395141]
loss: 2.5061446081543943e-06    d: [0.43226238]    y: [0.43002357]
loss: 1.873030291724777e-05    d: [-0.81380058]    y: [-0.80768007]
# iters_num 3000

loss: 1.0231756688242826e-07    d: [-0.29761864]    y: [-0.29716628]
loss: 1.2201090155677522e-08    d: [-0.56307233]    y: [-0.56322854]
loss: 4.038245317808e-11    d: [-0.65766776]    y: [-0.65765877]
loss: 1.0126131715021873e-08    d: [0.13182648]    y: [0.13168417]
loss: 6.953992391169471e-08    d: [0.49909101]    y: [0.49871807]
loss: 5.583833319530257e-08    d: [0.9518317]    y: [0.95149752]
loss: 1.0065416440921327e-07    d: [0.97784112]    y: [0.97739245]
loss: 1.703353457275218e-08    d: [-0.58880346]    y: [-0.58861889]
loss: 5.262281268423154e-08    d: [-0.78351093]    y: [-0.78383534]
loss: 2.609182826217252e-10    d: [-0.49909101]    y: [-0.49906816]
loss: 5.712849721366755e-08    d: [0.21857331]    y: [0.21823529]
loss: 9.126545094823052e-08    d: [-0.33938943]    y: [-0.3389622]
loss: 1.0304339135480852e-07    d: [-0.43793098]    y: [-0.43747701]
loss: 1.144575297280925e-07    d: [-0.33346065]    y: [-0.3329822]
loss: 4.4870288965271345e-08    d: [-0.99639027]    y: [-0.9960907]
loss: 4.5188451220634575e-08    d: [0.88624247]    y: [0.88654309]
loss: 2.31662693165207e-08    d: [-0.92833248]    y: [-0.92811723]
loss: 7.877136644355469e-10    d: [-0.52075286]    y: [-0.52079255]
loss: 8.254988700714719e-09    d: [-0.55262221]    y: [-0.5527507]
loss: 8.291678208202154e-08    d: [0.47711265]    y: [0.47670543]
loss: 3.515702303044461e-08    d: [0.60896952]    y: [0.60923469]
loss: 4.634967317333815e-08    d: [-0.94587102]    y: [-0.94556656]
loss: 9.366188781112265e-08    d: [0.27953518]    y: [0.27910237]
loss: 7.187670231689781e-08    d: [0.73863456]    y: [0.73901371]
loss: 2.5175272023289945e-08    d: [-0.00629574]    y: [-0.00607135]
loss: 4.238282860506185e-08    d: [0.54208448]    y: [0.54179333]
loss: 5.364268767272118e-08    d: [0.99781582]    y: [0.99748827]
loss: 7.761284716402532e-08    d: [-0.96441607]    y: [-0.96402208]
loss: 7.29946476556948e-08    d: [0.07547747]    y: [0.07509539]
loss: 5.3747292245554934e-12    d: [0.66239735]    y: [0.66240063]
loss: 3.917648631593904e-08    d: [-0.54736419]    y: [-0.54708428]
loss: 1.13968239999955e-07    d: [0.99393675]    y: [0.99345932]
loss: 1.8916287869206871e-10    d: [0.96441607]    y: [0.96443552]
loss: 6.104853811910363e-08    d: [-0.22471249]    y: [-0.22436306]
loss: 3.374712152494352e-08    d: [-0.99393675]    y: [-0.99367695]
loss: 7.459208770219936e-08    d: [-0.71705202]    y: [-0.71743827]
loss: 3.334659961238206e-09    d: [0.8649742]    y: [0.86505587]
loss: 1.0502769561774363e-07    d: [-0.11933469]    y: [-0.11887638]
loss: 3.740289231636632e-08    d: [-0.40941891]    y: [-0.4091454]
loss: 1.1571745806161676e-07    d: [0.39789889]    y: [0.39741782]
loss: 1.321914651353706e-08    d: [0.98611478]    y: [0.98595218]
loss: 6.824737983518538e-08    d: [-0.0691982]    y: [-0.06882875]
loss: 7.747335301914925e-08    d: [0.35709413]    y: [0.3567005]
loss: 4.2013170889456336e-08    d: [0.99583607]    y: [0.9955462]
loss: 2.15269655533141e-08    d: [-0.92597363]    y: [-0.92618113]
loss: 6.817128658014687e-08    d: [0.36882689]    y: [0.36845764]
loss: 1.1810524119654299e-08    d: [0.91617219]    y: [0.9160185]
loss: 1.5115146954173403e-09    d: [0.52611726]    y: [0.52617224]
loss: 8.054071771612444e-08    d: [0.96606148]    y: [0.96566013]
loss: 1.9560182092134635e-08    d: [0.43793098]    y: [0.43773319]
loss: 2.3651142874410942e-09    d: [-0.86811636]    y: [-0.86818514]
loss: 7.426592869320904e-08    d: [-0.99975723]    y: [-0.99937184]
loss: 3.3690045164666617e-08    d: [0.77562491]    y: [0.77588449]
loss: 4.242873255556768e-09    d: [0.04405617]    y: [0.04414829]
loss: 9.775107383336294e-09    d: [0.71705202]    y: [0.71719184]
loss: 4.388321806850582e-10    d: [0.8773359]    y: [0.87736552]
loss: 3.008800038081094e-08    d: [0.91363079]    y: [0.9138761]
loss: 3.4353135859984793e-09    d: [-0.97784112]    y: [-0.97775823]
loss: 8.641444917021051e-10    d: [0.96101064]    y: [0.96105221]
loss: 8.711809080427124e-08    d: [0.26742375]    y: [0.26700633]
loss: 5.020776597501214e-08    d: [-0.87122411]    y: [-0.871541]
loss: 2.838017375638594e-08    d: [-0.91617219]    y: [-0.91641043]
loss: 1.5567226852673516e-09    d: [0.87122411]    y: [0.87127991]
loss: 9.80377783788852e-09    d: [0.98394564]    y: [0.98380562]
loss: 4.148061613211945e-08    d: [0.4036669]    y: [0.40337887]
loss: 8.246547424764583e-08    d: [0.0880268]    y: [0.08762068]
loss: 3.519939115211657e-08    d: [0.81012572]    y: [0.81039105]
loss: 3.3475921950851644e-08    d: [0.41515469]    y: [0.41489593]
loss: 3.920213266306929e-08    d: [-0.99524241]    y: [-0.9949624]
loss: 6.67659132947818e-09    d: [-0.94789551]    y: [-0.94801107]
loss: 2.725939613621103e-08    d: [0.16916853]    y: [0.16893503]
loss: 5.9019549688727115e-08    d: [-0.95374324]    y: [-0.95339967]
loss: 7.404264206970432e-08    d: [-0.72577151]    y: [-0.72615633]
loss: 1.950089841546687e-08    d: [0.74286391]    y: [0.7430614]
loss: 9.170656345180158e-09    d: [-0.94380904]    y: [-0.94394447]
loss: 1.8342443303680584e-08    d: [-0.83516734]    y: [-0.83535887]
loss: 4.0168629528711154e-08    d: [-0.94170965]    y: [-0.94142621]
loss: 1.0438172705794946e-07    d: [0.32156366]    y: [0.32110676]
loss: 7.965196525121474e-08    d: [-0.48263615]    y: [-0.48223703]
loss: 4.522391104049084e-08    d: [0.03776568]    y: [0.03746493]
loss: 8.671646127119746e-08    d: [0.34530476]    y: [0.34488831]
loss: 3.005746961010373e-08    d: [0.56307233]    y: [0.56282714]
loss: 3.3448244853359074e-08    d: [0.90843947]    y: [0.90869812]
loss: 9.740854716974263e-08    d: [0.99940055]    y: [0.99895917]
loss: 5.2790241459135134e-08    d: [0.85534252]    y: [0.85566745]
loss: 1.3438734027099605e-08    d: [0.93739898]    y: [0.93756292]
loss: 1.0825441262636896e-07    d: [0.99738016]    y: [0.99691486]
loss: 7.346796784117262e-08    d: [-0.6992734]    y: [-0.69965672]
loss: 9.629719356127723e-08    d: [-0.10682399]    y: [-0.10638513]
loss: 4.97443282272803e-09    d: [-0.54208448]    y: [-0.54218422]
loss: 7.137974552190541e-08    d: [0.9995987]    y: [0.99922087]
loss: 1.1991319740505605e-07    d: [0.29761864]    y: [0.29712892]
loss: 3.111699813996102e-08    d: [0.99322482]    y: [0.99297535]
loss: 9.57392195916358e-08    d: [0.33346065]    y: [0.33302307]
loss: 5.181836972664086e-08    d: [-0.83168816]    y: [-0.83201008]
loss: 1.1452214926872929e-08    d: [-0.98504973]    y: [-0.98489839]
loss: 5.3815470676520537e-08    d: [-0.64332332]    y: [-0.6436514]
loss: 1.0539082479501495e-07    d: [0.43226238]    y: [0.43180327]
loss: 3.266434009851389e-08    d: [-0.81380058]    y: [-0.81405617]
# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


4.双方向RNN
要点： 
① 双方向RNN(Bidirectional RNN)＝過去の情報を次の層で反映するだけでなく、未来の情報を加味したモデル
② 実用例＝文書の推敲や機械翻訳（学習には未来の情報が取得できるデータが必要）
③ 特徴量＝順方向と逆方向に伝播したときの中間層を表現を合わせたもの

5.Seq2Seq
要点：
① Seq2Seq＝機械翻訳に使われるニューラルネットワーク。エンコーダ、デコーダモデルの一種
② エンコーダ＝データをトークンに区切って渡す構造
③ デコーダ＝アウトプットデータをトークンごとに生成する構造
④ MLM＝与えられたデータの一部分を落とし、その部分に入れるべきベクトルを学習
確認テスト：
Q.seq2seqについて説明しているものを述べよ？
A.(2)RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる

6.Word2vec 
要点：
① Word2vec＝単語や文字列をベクトル表現にする手法（全単語にベクトルを設定する場合、単語数だけone-hotベクトルが必要）
② 構造＝文章で使われている単語をリストアップ→単語に番号を振る→単語をベクトル化する
③ 注意点＝変換表を作成する必要あり

7.Attention Mechanism
要点：
① Attention Mechanism＝一文の中で重要な単語を見分けて抜き出す方法。単語の関連度を学習する
② 特徴＝長文に強い（seq2seqで対応が難しい長い文章に強い）
③ 構造＝文章が長くなるほどシーケンスの内部表現の次元を大きくする。翻訳する前後の単語の関係性を学習
確認テスト：
Q.RNN、word2vec、seq2seq、Attentionの違い？
A.RNN＝時系列データ
word2vec＝単語の分散表現を得る手法
seq2seq＝１つの時系列データから他の時系列データを出力する方法
Attention＝１つの時系列データの関連性に重みを作る手法
