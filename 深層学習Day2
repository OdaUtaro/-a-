1.勾配消失問題
要点：
① 勾配消失問題＝ニューラルネットワークの中間層を増やすことによって発生する問題。下位層に伝わる誤差が少なくなる
② 原因＝誤差伝播法による微分の連鎖率が増えること＋活性化関数の微分の最大値が小さい
③ 解決方法＝活性化関数の選択/重みの初期値設定/バッチ正規化
④ 勾配消失問題が起こりにくい活性化関数＝ReLU関数など
⑤ 重みの初期値を設定＝重みの要素を前の層のノード数の平方根で除算した値。S字関数はXavier、Relu関数はHeを使う
実装演習結果：
確認テスト：
Q.連鎖率の原理を使い、dz/dxを求めよ？
A.dz/dx = dz/dt * dt/dx
dz/dt = 2t
dt/dx = 1
dz/dx = dz/dt * dt/dx = 2t = 2(x + y)
Q.シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものは？
A. (2)0.25
Q. 重みの初期値に0を設定するとどのような問題が発生するのか？
A. すべての重みの値が均一に更新されるため、多数の重みをもつ意味がなくなる
その他：

2.学習率最適化手法
要点：
実装演習結果：
確認テスト：
その他：

3.過学習
要点：
実装演習結果：
確認テスト：
その他：

4.畳み込みニューラルネットワーク
要点：
実装演習結果：
確認テスト：
その他：

5.最新のCNN  
要点：
実装演習結果：
確認テスト：
その他：
