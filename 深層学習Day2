1.勾配消失問題
要点：
① 勾配消失問題＝ニューラルネットワークの中間層を増やすことによって発生する問題。下位層に伝わる誤差が少なくなる
② 原因＝誤差伝播法による微分の連鎖率が増えること＋活性化関数の微分の最大値が小さい
③ 解決方法＝活性化関数の選択/重みの初期値設定/バッチ正規化
④ 勾配消失問題が起こりにくい活性化関数＝ReLU関数(勾配消失問題を回避し、スパース化に貢献）
⑤ 重みの初期値を設定＝重みの要素を前の層のノード数の平方根で除算した値。S字関  数はXavier、Relu関数はHeを使う
⑥ バッチ正規化＝ミニバッチ単位で入力値のデータの偏りを抑制する手法。過学習を防ぐ、中間層の重みの更新が安定しスピードアップする
実装演習結果：
確認テスト：
Q.連鎖率の原理を使い、dz/dxを求めよ？
A.dz/dx = dz/dt * dt/dx
dz/dt = 2t
dt/dx = 1
dz/dx = dz/dt * dt/dx = 2t = 2(x + y)
Q.シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものは？
A. (2)0.25
Q. 重みの初期値に0を設定するとどのような問題が発生するのか？
A. すべての重みの値が均一に更新されるため、多数の重みをもつ意味がなくなる
Q. パッチ正規化の効果？
A. 重みの更新が安定、過学習を防ぐ
実装例：
・活性化関数の選択
MINST　Dataでのニューラルネットワーク(全体はDay1レポートに記載）
class TwoLayerNet:
        self.layers['Relu1'] = Relu()
→上記部分をSigmoid関数とRelu関数で比較
Sigmoid関数結果
0.10218333333333333 0.101
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135
0.11236666666666667 0.1135

Relu関数結果
0.09026666666666666 0.0836
0.0993 0.1032
0.09751666666666667 0.0974
0.09751666666666667 0.0974
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098
0.09871666666666666 0.098

2.学習率最適化手法
要点：
① 学習率最適化手法＝モメンタム、AdaGrad、RMSProp、Adam。初期は学習率を大きくし、徐々に小さくする
② モメンタム＝新たに慣性という項を追加する。大域的最適解を見つけることができる
③ AdaGrad＝勾配量の二乗を覚えながら更新していく。勾配の緩やかな斜面の最適化に適している
④ RMSProp＝昔の勾配情報をどの程度参照するのかを調整できる。大域最適解となる。Adagradの進化版
⑤ Adam＝モメンタムおよびRMSPropのメリットを孕んだアルゴリズム
実装演習結果：
確認テスト：
その他：

3.過学習
要点：
① 過学習＝学習用のデータの学習が進みすぎ、検証用のデータに対する精度が悪くなること
② 過学習を抑制する方法＝正則化（L1、L2）、ドロップアウト
③ L1正則化＝p1ノルム（マンハッタン距離）を使う
④ L2正則化＝p2ノルム（ユーグリット距離）を使う
⑤ ドロップアウト＝部分的にニューラルネットワークを切断して学習を行う
実装演習結果：
確認テスト：
Q. L1正則化を表しているグラフはどちらか？
A. 右の正則化項の等高線がダイヤ型の方
その他：

4.畳み込みニューラルネットワーク
要点：
① 畳み込みニューラルネットワーク＝画像処理、識別を使うときに主に扱う。次元のつながりのあるデータを扱える
② 画像のCNNの構造＝入力層→畳み込み層→プーリング層→畳み込み層→プーリング層→全結合層→出力層
③ LeNet＝32*32の画像から10個の出力を行う
④ 畳込み層＝入力値にフィルターをかけ、出力した値にバイアスを加え、データを畳み込む
⑤ 全結合層＝人間が欲しい結果を求める層
⑥ 畳み込み演算を利用する理由＝縦、横などの情報を考慮した上での出力を行うため
⑦ プーリング層＝対象領域の最大値または平均を取得し、出力する
実装演習結果：
確認テスト：
Q. サイズ6*6の入力画像をサイズ2*2のフィルタで畳み込んだ時の出力画像のサイズを答えよ(パディング、ストライドは1)？
A. 6*6
公式　高さ＝画像の高さ＋2*パディング高さ-フィルタの高さ/ストライド+1
幅＝画像の幅＋2*パディング幅-フィルタの幅/ストライド+1
その他：

5.最新のCNN  
要点：
① 最新のCNN＝AlexNet
② AlexNet＝５層の畳み込み層及びプーリングなど、それに続く３層の全結合層から構成される
③ 入力層＝224*224の画像
④ 出力層＝1000
⑤ １次元の行列にする方法＝グローバルマックスプーリング or グローバルアベレージプーリング
実装演習結果：
確認テスト：
その他：
